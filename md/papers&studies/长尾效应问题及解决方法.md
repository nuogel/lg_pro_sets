# 长尾问题调研
## 概述
长尾问题指的是数据集类别不平衡，某些类别占了整个样本数量的绝大部份，其余类别只占了很小部分，导致模型训练中更多去关注样本多的类别，少样本的类别就几乎没对模型产生影响。在总体的结果上看，可能模型表现尚可，但是应用到少样本类别时，效果就会非常差。
## 现状分析
当前目标检测生产环境中，会经常遇到长尾问题（数据不平衡），比如在公司项目加油站场景中，每天抽烟的事件会远远小于打电话的事件，如果将抽烟和打电话用同一模型进行训练，训练出来的模型则会更多关注到打电话类别上。再比如工业检测中，某些缺陷检测类别发生概率极小，如果和其他缺陷一起训练则会导致长尾问题。
当前主流方法主要从五个方向入手：
## 数据集
1. 数据增强 + 重采样
对长尾的类别做 重采样+数据增强，包括图像拼接，亮度饱和调整，mixup，mosiac等
2. 目标拼接
利用Rect标注或者mask标注将目标实体抠出，经过仿射变换拼接到新的背景图像上，再做边缘平滑处理，如Copypaste等。
## 网络结构
 1. 学习任务解耦
将特征学习和分类器学习解耦，把不平衡学习分为两个阶段，在特征学习阶段正常采样，在分类器学习阶段平衡采样，可以带来更好的长尾学习结果
论文地址：https://arxiv.org/abs/1910.09217
代码：https://github.com/facebookresearch/classifier-balancing
这篇论文认为数据不均衡问题不会影响高质量Representations（backbone）的学习。即random sampling策略往往会学到泛化性更好的representations；但会影响到classification（head）的学习，因此训练分为两个阶段，第一个阶段就是random sampling正常训练，第二阶段锁定Representations，针对classification做class-balanced sampling训练。
论文结果：finetune阶段，特征学习和分类器学习解耦比不解耦在少样本的类别ap提升2.5%，中数量样本提升0.9%，在Places-LT数据集上，使用τ -normalized解耦方式训练的模型，在中数量和少数量样本分别比focal loss提升5.9%和9.4%

## 损失函数
1. 通过Balanced Group Softmax方法\
在长尾分布中,样本数量较少的类别，其对应分类器权重的模也较小，因此训练模型中尾类样本对应的神经元只有很少的机会被激活.。因为这些尾类对应的｜W｜ 较小，因此在通过softmax层之后会被放大，并进一步导致这些少样本类别的物体难以检测。该篇文章提出一种方法，使样本少的类别在分类器的中获得更多的权重｜W｜。
论文地址：https://arxiv.org/pdf/2006.10408.pdf
文章的解决办法是将所有类别的样本根据其样本数量分成互不相交的若干组，然后分别做softmax。即：样本多的类别放在一个组中，样本数量少的类别放入一个组中。此时，由于同组内类别的样本数量相近，则在训练时就不会产生权值抑制现象。
论文结果：在长尾数据集LVIS上，使用了本文方法在ResNeXt-101 版本的Faster-rcnn基础上提升了3.2%，在Cascade R-CNN基础上提升了5.6%
2. seesaw loss\
论文地址：https://arxiv.org/abs/2008.10032
该文章指出了尾部类别上的正负样本梯度的不平衡是影响长尾检测性能的关键因素之一。 Seesaw Loss 针对性地调整施加在任意一个类别上的负样本梯度。给定一个尾部类别和一个相对更加高频的类别，高频类施加在尾部类上的负样本梯度将根据两个类别在训练过程中累计样本数的比值进行减弱。同时为了避免因负样本梯度减弱而增加的误分类的风险，Seesaw Loss 根据每个样本是否被误分类动态地补充负样本梯度，损失函数如下：

对于第i类物体，施加在第j类的负梯度为：
 

其中，Sij就像一个平衡系数，通过调整Sij，就可以调整负梯度。在 Seesaw Loss 的设计中，一方面需要考虑类别间样本分布的关系（class-wise），并据此减少头部类别对尾部类别的"惩罚" （负样本梯度）；另一方面，盲目减少对尾部类别的惩罚会增加错误分类的风险，因为部分误分类的样本受到的惩罚变小了，因此对于那些在训练过程中误分类的样本我们需要保证其受到足够的"惩罚"。在每次训练迭代中，通过样本的数量动态控制Sij，从而达到平衡各类样本产生的影响。
但是论文里针对的是Instance Segmentation，作者在知乎上说对于目标检测，尚没有实验结论
论文结果：在LVIS v1数据集上，以Mask R-CNN和Cascade Mask R-CNN为基准，Seesaw Loss+Norm Mask策略分别比上文提到的Balanced Group Softmax分组方法提高1.5%和1.7%（随机采样），2.3%和3.1%（平衡采样）

## 特征对比，度量学习
只可用于分类任务，模型训练输入为正负样本pair，模型优化方向为使让正样本之间度量较小，正负样本之间度量较大，这样对于样本较少的实体，就可以把他们的特征存入database，待检测的样本提取特征后进行比对，这样在较少样本的情况下也能完成任务。
## GAN网络
工业检测中，使用GAN网络生成缺陷样本数据集，弥补工业检测负样本严重缺失的情况。但是GAN网络的训练本身需要大量样本，所以可行性还需要评估。

总结建议：
（1）在AI自动化平台开放 【重采样】+ 【手动数据增强】+ 【自定义拼接】功能，集成边缘平滑算法，将处理好的数据归档到标准数据集里
（2）平台增加 seesaw loss 损失函数

Reference
[1] B Kang，S Xie，M Rohrbach，Z Yan，A Gordo，J Feng，Y Kalantidis  Decoupling Representation and Classifier for Long-Tailed Recognition [J/OL] https://arxiv.org/abs/1910.09217
[2] Yu Li, Tao Wang, Bingyi Kang, Sheng Tang, Chunfeng Wang, Jintao Li, Jiashi Feng  Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax [J/OL] https://arxiv.org/abs/2006.10408
[3] Jiaqi Wang, Wenwei Zhang, Yuhang Zang, Yuhang Cao, Jiangmiao Pang, Tao Gong, Kai Chen, Ziwei Liu, Chen Change Loy, Dahua Lin Seesaw Loss for Long-Tailed Instance Segmentation  [J/OL] https://arxiv.org/abs/2008.10032
